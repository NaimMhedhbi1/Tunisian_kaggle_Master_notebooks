{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-09-22T16:20:47.464303Z",
     "iopub.status.busy": "2020-09-22T16:20:47.463398Z",
     "iopub.status.idle": "2020-09-22T16:20:47.469284Z",
     "shell.execute_reply": "2020-09-22T16:20:47.468630Z"
    },
    "papermill": {
     "duration": 0.034882,
     "end_time": "2020-09-22T16:20:47.469440",
     "exception": false,
     "start_time": "2020-09-22T16:20:47.434558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019749,
     "end_time": "2020-09-22T16:20:47.510896",
     "exception": false,
     "start_time": "2020-09-22T16:20:47.491147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this tutorial I will focus on these 3 different strategies : \n",
    "1. 1- Bag-of-words( with Tf-Idf) : used with simple machine learning algorithm \n",
    "1. 2-Word Embedding (with Word2vec) : used with deep learning neural network \n",
    "1. 3- Bert : used with transfer learning from attention-based transformers. \n",
    "\n",
    "\n",
    "NLP : it is about programming computers to process and analyze large amounts of natural language data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-09-22T16:20:47.557082Z",
     "iopub.status.busy": "2020-09-22T16:20:47.556248Z",
     "iopub.status.idle": "2020-09-22T16:20:58.521882Z",
     "shell.execute_reply": "2020-09-22T16:20:58.521029Z"
    },
    "papermill": {
     "duration": 10.991189,
     "end_time": "2020-09-22T16:20:58.522023",
     "exception": false,
     "start_time": "2020-09-22T16:20:47.530834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "#First of all, I need to import the following libraries:\n",
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "## for bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:20:58.574026Z",
     "iopub.status.busy": "2020-09-22T16:20:58.573291Z",
     "iopub.status.idle": "2020-09-22T16:20:58.625597Z",
     "shell.execute_reply": "2020-09-22T16:20:58.626227Z"
    },
    "papermill": {
     "duration": 0.083662,
     "end_time": "2020-09-22T16:20:58.626423",
     "exception": false,
     "start_time": "2020-09-22T16:20:58.542761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtf = pd.read_csv(\"../input/nlp-getting-started/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020268,
     "end_time": "2020-09-22T16:20:58.667303",
     "exception": false,
     "start_time": "2020-09-22T16:20:58.647035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "the univariate distribution of the target : the labels frequency with a bar plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:20:58.711923Z",
     "iopub.status.busy": "2020-09-22T16:20:58.711290Z",
     "iopub.status.idle": "2020-09-22T16:20:58.924581Z",
     "shell.execute_reply": "2020-09-22T16:20:58.925218Z"
    },
    "papermill": {
     "duration": 0.237621,
     "end_time": "2020-09-22T16:20:58.925407",
     "exception": false,
     "start_time": "2020-09-22T16:20:58.687786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"target\", fontsize=12)\n",
    "dtf[\"target\"].reset_index().groupby(\"target\").count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02131,
     "end_time": "2020-09-22T16:20:58.969187",
     "exception": false,
     "start_time": "2020-09-22T16:20:58.947877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "the dataset is balanced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021461,
     "end_time": "2020-09-22T16:20:59.012446",
     "exception": false,
     "start_time": "2020-09-22T16:20:58.990985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before explaining and building the models, I am going to give an example of preprocessing by cleaning text, removing stop words, and applying lemmatization. I will write a function and apply it to the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:20:59.059155Z",
     "iopub.status.busy": "2020-09-22T16:20:59.058403Z",
     "iopub.status.idle": "2020-09-22T16:20:59.069463Z",
     "shell.execute_reply": "2020-09-22T16:20:59.070054Z"
    },
    "papermill": {
     "duration": 0.036155,
     "end_time": "2020-09-22T16:20:59.070217",
     "exception": false,
     "start_time": "2020-09-22T16:20:59.034062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess a string.\n",
    ":parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param lst_stopwords: list - list of stopwords to remove\n",
    "    :param flg_stemm: bool - whether stemming is to be applied\n",
    "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    ":return\n",
    "    cleaned text\n",
    "'''\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and   \n",
    "    #characters and then strip\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2020-09-22T16:20:59.116813Z",
     "iopub.status.busy": "2020-09-22T16:20:59.116128Z",
     "iopub.status.idle": "2020-09-22T16:20:59.135446Z",
     "shell.execute_reply": "2020-09-22T16:20:59.134713Z"
    },
    "papermill": {
     "duration": 0.04379,
     "end_time": "2020-09-22T16:20:59.135583",
     "exception": false,
     "start_time": "2020-09-22T16:20:59.091793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lst_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02523,
     "end_time": "2020-09-22T16:20:59.186700",
     "exception": false,
     "start_time": "2020-09-22T16:20:59.161470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So let's apply the function and store the result in a new column named 'text_clean' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:20:59.245052Z",
     "iopub.status.busy": "2020-09-22T16:20:59.244188Z",
     "iopub.status.idle": "2020-09-22T16:21:03.273247Z",
     "shell.execute_reply": "2020-09-22T16:21:03.274231Z"
    },
    "papermill": {
     "duration": 4.062443,
     "end_time": "2020-09-22T16:21:03.274479",
     "exception": false,
     "start_time": "2020-09-22T16:20:59.212036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \n",
       "0       1         deed reason earthquake may allah forgive u  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  resident asked shelter place notified officer ...  \n",
       "3       1  13000 people receive wildfire evacuation order...  \n",
       "4       1  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:03.383321Z",
     "iopub.status.busy": "2020-09-22T16:21:03.382061Z",
     "iopub.status.idle": "2020-09-22T16:21:03.388271Z",
     "shell.execute_reply": "2020-09-22T16:21:03.388838Z"
    },
    "papermill": {
     "duration": 0.078116,
     "end_time": "2020-09-22T16:21:03.389031",
     "exception": false,
     "start_time": "2020-09-22T16:21:03.310915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## split dataset\n",
    "dtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.3)\n",
    "## get target\n",
    "y_train = dtf_train[\"target\"].values\n",
    "y_test = dtf_test[\"target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023351,
     "end_time": "2020-09-22T16:21:03.436056",
     "exception": false,
     "start_time": "2020-09-22T16:21:03.412705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bag-of-Words******\n",
    "\n",
    "So Bag of words works like this  : it builds a vocabulary from a corpus of documents and then counts how many times the words appear in each document. \n",
    "so each word becomes a feature and a  document is represented by a vector with the same lenghth of the vocabulary. \n",
    "*the Feature matrix shape = Number of documents * length of vocabulary.*\n",
    "\n",
    "As you can image , this approach causes  a huge sparse matrix ; a significant dimensionality problem : the more documents you have the larger is the vocabulary. That's why the bag of words model is usually preceded by an important preprocessing (word claning , stop words removals , stemming/lemmatization) aimed to reduce the dimensionnality problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023716,
     "end_time": "2020-09-22T16:21:03.483549",
     "exception": false,
     "start_time": "2020-09-22T16:21:03.459833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I skipped the part of TFIDF ! I will expalain it later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023154,
     "end_time": "2020-09-22T16:21:03.531475",
     "exception": false,
     "start_time": "2020-09-22T16:21:03.508321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So after splitting the data we will focus now on ***Feature Engineering***, which is the process of creating features by extracting information from the data. I am going to use the Tf-Idf vectorizer with a limit of 10,000 words (so the length of my vocabulary will be 10k), capturing unigrams (i.e. “new” and “york”) and bigrams (i.e. “new york”) #if we use trigrams it will be for example \"new york city\". I will provide the code for the classic count vectorizer as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:03.582751Z",
     "iopub.status.busy": "2020-09-22T16:21:03.581643Z",
     "iopub.status.idle": "2020-09-22T16:21:03.587315Z",
     "shell.execute_reply": "2020-09-22T16:21:03.587848Z"
    },
    "papermill": {
     "duration": 0.033124,
     "end_time": "2020-09-22T16:21:03.588019",
     "exception": false,
     "start_time": "2020-09-22T16:21:03.554895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Count (classic BoW)\n",
    "vectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "## Tf-Idf (advanced variant of BoW)\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.02294,
     "end_time": "2020-09-22T16:21:03.634288",
     "exception": false,
     "start_time": "2020-09-22T16:21:03.611348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:03.711272Z",
     "iopub.status.busy": "2020-09-22T16:21:03.684220Z",
     "iopub.status.idle": "2020-09-22T16:21:04.424459Z",
     "shell.execute_reply": "2020-09-22T16:21:04.423662Z"
    },
    "papermill": {
     "duration": 0.766795,
     "end_time": "2020-09-22T16:21:04.424591",
     "exception": false,
     "start_time": "2020-09-22T16:21:03.657796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = dtf_train[\"text_clean\"]\n",
    "vectorizer.fit(corpus)\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024236,
     "end_time": "2020-09-22T16:21:04.473380",
     "exception": false,
     "start_time": "2020-09-22T16:21:04.449144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In order to know the position of a certain word, we can look it up in the vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:04.530798Z",
     "iopub.status.busy": "2020-09-22T16:21:04.529887Z",
     "iopub.status.idle": "2020-09-22T16:21:04.534955Z",
     "shell.execute_reply": "2020-09-22T16:21:04.534186Z"
    },
    "papermill": {
     "duration": 0.037099,
     "end_time": "2020-09-22T16:21:04.535095",
     "exception": false,
     "start_time": "2020-09-22T16:21:04.497996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3031"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"forest\"\n",
    "dic_vocabulary[word]\n",
    "#If the word exists in the vocabulary, \n",
    "#this command prints a number N, \n",
    "#meaning that the Nth feature of the matrix is that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024343,
     "end_time": "2020-09-22T16:21:04.584585",
     "exception": false,
     "start_time": "2020-09-22T16:21:04.560242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**reduce the matrix dimensionality****\n",
    "in order to reduce the dimensionality of our matrix ! [Feature matrix shape: Number of documents x Length of vocabulary ] we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:\n",
    "* treat each category as binary (for example, the “Tech” category is 1 for the Tech news and 0 for the others);\n",
    "1. perform a Chi-Square test to determine whether a feature and the (binary) target are independent;\n",
    "1. keep only the features with a certain p-value from the Chi-Square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:04.638893Z",
     "iopub.status.busy": "2020-09-22T16:21:04.638078Z",
     "iopub.status.idle": "2020-09-22T16:21:04.819675Z",
     "shell.execute_reply": "2020-09-22T16:21:04.818522Z"
    },
    "papermill": {
     "duration": 0.210308,
     "end_time": "2020-09-22T16:21:04.819854",
     "exception": false,
     "start_time": "2020-09-22T16:21:04.609546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import feature_selection \n",
    "y = dtf_train[\"target\"]\n",
    "X_names = vectorizer.get_feature_names()\n",
    "p_value_limit = 0.95\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "len(X_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:04.896953Z",
     "iopub.status.busy": "2020-09-22T16:21:04.895635Z",
     "iopub.status.idle": "2020-09-22T16:21:04.908588Z",
     "shell.execute_reply": "2020-09-22T16:21:04.909376Z"
    },
    "papermill": {
     "duration": 0.050955,
     "end_time": "2020-09-22T16:21:04.909584",
     "exception": false,
     "start_time": "2020-09-22T16:21:04.858629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0:\n",
      "  . selected features: 237\n",
      "  . top features: hiroshima,california,fire,mh370,suicide,wildfire,bombing,killed,im,northern\n",
      " \n",
      "# 1:\n",
      "  . selected features: 237\n",
      "  . top features: hiroshima,california,fire,mh370,suicide,wildfire,bombing,killed,im,northern\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for cat in np.unique(y):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\"  . selected features:\",\n",
    "         len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "    print(\"  . top features:\", \",\".join(\n",
    "dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025156,
     "end_time": "2020-09-22T16:21:04.961451",
     "exception": false,
     "start_time": "2020-09-22T16:21:04.936295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#training the model ! \n",
    "Naive Bayes algorithm: a probabilistic classifier that makes use of Bayes’ Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:05.017703Z",
     "iopub.status.busy": "2020-09-22T16:21:05.016640Z",
     "iopub.status.idle": "2020-09-22T16:21:05.021499Z",
     "shell.execute_reply": "2020-09-22T16:21:05.022024Z"
    },
    "papermill": {
     "duration": 0.035102,
     "end_time": "2020-09-22T16:21:05.022215",
     "exception": false,
     "start_time": "2020-09-22T16:21:04.987113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025542,
     "end_time": "2020-09-22T16:21:05.073988",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.048446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I’m going to train this classifier on the feature matrix and then test it on the transformed test set. To that end, I need to build a scikit-learn pipeline: a sequential application of a list of transformations and a final estimator. Putting the Tf-Idf vectorizer and the Naive Bayes classifier in a pipeline allows us to transform and predict test data in just one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-22T16:21:05.130229Z",
     "iopub.status.busy": "2020-09-22T16:21:05.129210Z",
     "iopub.status.idle": "2020-09-22T16:21:05.294737Z",
     "shell.execute_reply": "2020-09-22T16:21:05.295263Z"
    },
    "papermill": {
     "duration": 0.19537,
     "end_time": "2020-09-22T16:21:05.295461",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.100091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)\n",
    "## test\n",
    "X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025793,
     "end_time": "2020-09-22T16:21:05.347055",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.321262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can now evaluate the performance of the Bag-of-Words model, I will use the following metrics:****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025809,
     "end_time": "2020-09-22T16:21:05.398981",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.373172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Accuracy** is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition:\n",
    "Accuracy in Machine Learning\n",
    "The accuracy of a machine learning classification algorithm is one way to measure how often the algorithm classifies a data point correctly. Accuracy is the number of correctly predicted data points out of all the data points. More formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives. A true positive or true negative is a data point that the algorithm correctly classified as true or false, respectively. A false positive or false negative, on the other hand, is a data point that the algorithm incorrectly classified. For example, if the algorithm classified a false data point as true, it would be a false positive. Often, accuracy is used along with precision and recall, which are other metrics that use various ratios of true/false positives/negatives. Together, these metrics provide a detailed look at how the algorithm is classifying data points. \n",
    "\n",
    "Example\n",
    "Consider a classification algorithm that decides whether an email is spam or not. The algorithm is trained, and we want to see how well it performs on a set of ten emails it has never seen before. Of the ten emails, six are not spam and four are spam. The algorithm classifies three of the messages as spam, of which two are actually spam, and one is not spam. In the table, the true positives (the emails that are correctly identified as spam) are colored in green, the true negatives (the emails that are correctly identified as not spam) are colored in blue, the false positives (the not spam emails that are incorrectly classified as spam) are colored in red, and the false negatives (the spam emails that are incorrectly identified as not spam) are colored in orange. There are two true positives, five true negatives, two false negatives, and one false positive. Using the formula for accuracy, we get: \n",
    "\n",
    "This algorithm has 70% accuracy classifying emails as spam or not. \n",
    "![](https://images.deepai.org/django-summernote/2019-05-09/ad392084-735b-432a-bdf0-b4b56a455de3.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025135,
     "end_time": "2020-09-22T16:21:05.449983",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.424848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "What is a **Confusion Matrix**?\n",
    "The million dollar question – what, after all, is a confusion matrix?\n",
    "\n",
    "A **Confusion matrix** is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n",
    "\n",
    "For a binary classification problem, we would have a 2 x 2 matrix as shown below with 4 values:\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Basic-Confusion-matrix.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024931,
     "end_time": "2020-09-22T16:21:05.500365",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.475434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**AUC - ROC** curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n",
    "The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n",
    "![](https://miro.medium.com/max/361/1*pk05QGzoWhCgRiiFbz-oKQ.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025137,
     "end_time": "2020-09-22T16:21:05.551733",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.526596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Precision is how close measure values are to each other, basically how many decimal places are at the end of a given measurement.  Precision does matter.  Accuracy is how close a measure value is to the true value.  Accuracy matters too, but it’s best when measurements are both precise and accurate.\n",
    "\n",
    "Failure to understand the tension between precision and accuracy can have profound negative effects on how one processes data, and the final outcome of geospatial analysis.\n",
    "\n",
    "![](https://i0.wp.com/wp.stolaf.edu/it/files/2017/06/precsionvsaccuracy_crashcourse.png?resize=579%2C600&ssl=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024998,
     "end_time": "2020-09-22T16:21:05.602183",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.577185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recall is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity.\n",
    "\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "1\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025052,
     "end_time": "2020-09-22T16:21:05.652470",
     "exception": false,
     "start_time": "2020-09-22T16:21:05.627418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Credits to : \n",
    "1. 1- https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/ \n",
    "1. 2- https://deepai.org/researchers \n",
    "1. 3- https://wp.stolaf.edu/it/gis-precision-accuracy/ \n",
    "1. 4- https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 \n",
    "\n",
    "1. 5 https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 23.277908,
   "end_time": "2020-09-22T16:21:05.795891",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-22T16:20:42.517983",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
